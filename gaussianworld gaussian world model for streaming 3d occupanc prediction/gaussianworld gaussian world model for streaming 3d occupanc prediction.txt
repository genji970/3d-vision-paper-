current occupancy 
fusion  ego motion과 scene evolution

tri-perspective view : TPV

temporal information을 통합하면 single frame 3d occupancy prediction 성능을 올릴 수 있다.

대부분의 방법은 과거 scene 표현을 fuse한다. -> 추가 계산 비용 + 시나리오의 연속성 무시.

이 논문에서는 world model에 기반한 프레임워크를 제안한다. 

perception module은 각각의 frame의 scene 특징을 독립적으로 얻는다

cost volume이란? : 



현재 방법은,

perception               -> transformation ->                 fusion
(bev나 cost volume)    ego trajectory 기반으로 align     정렬된 특징을 fuse

이 방법은 inherent continuity와 simplicit 고려에 실패한다

시간적 변화에 대한 사전정보(prior)를 고려해야 multi frame 정보 결합이 갖는 의미를 포착하기 더 쉽다

 unified refinement layer : 하나의 통합된 레이어로, 여러 기능(과거의 진행과 새로운 정보의 인식)을 동시에 처리하는 층.

progression of historical gaussians:과거에 존재했던 gaussian 분포들의 시간적인 변화 혹은 누적된 상태를 모델링하는 것.

perception of newly completed gaussians:새로 생성되거나 업데이트된 gaussian 분포를 인식하는 것.

현재의 occupancy와 scene evolution을 결정하기 위해 refined gaussians를 활용한다.

현재 world model의 application은(자율주행의 경우), scene generation,planning, representation learning이 있다.

generativa models를 쓰기도한다.

ego motion과 scene evolution을 jointl modeling함으로서 world model은 효율적인 drive policy,를 학습할 수 있다.

world model은 sensor input , ego position을 인풋으로 받는다.

ego vehicle이 이동함에 따라, frame간의 ego centric representations이 서로 정렬되지 않는다

transformation module을 통해 ego vehicle의 ego trajectory를 기반으로 과거의 feature를 현재 frame에 맞게 align한다

local movements of dynamic objects :

gaussian world는 dynamic gaussian의 position을 업데이트함으로서 dynamic object의 local movements를 고려할 수 있다.

객체가 움직이면서 생기는 기존 안 보이는 area는 버리고 새 area는 임의로 초기화된 3d gaussian을 채워넣는다

encoder module과 refinement module은 같은 모델 아키텍쳐와 파라미터를 공유한다. -> 통합된 evolution layer에 통합되고 parallel하게 계산된다. 
 -> 이로 인해 gaussian world는 model의 단순성과 계산 효율성을 보일 수 있다.

-> 최종적으로, 잠재적인 misalignment(3d gaussian과 real world간의)를 해결하기 위해 또다른 refinement layers로 모든 3d gaussians의 특성들을 fine tune한다.


loss로는 cross entropy와 lovasz softmax를 학습과정에서 썼다.

single frame task에서 model을 pretrain헀다.

처음에 모델의 예측력이 좋지 않으므로 처음에는 시퀀스를 짧게하고(특정 확률로 과거 frame을 버림으로서) 점진적으로 증가시킨다

experiment

각 Gaussian은 다음과 같은 정보를 가짐:

1) 위치
2) 공분산 : 모양/방향성
3) 색상(RGB)
4) 불투명도(opacity)
5)학습 가능한 파라미터들

GaussianFormer는 최근 3D 비전 분야에서 주목받는 구조로, 3D Gaussian Splatting을 기반으로 Transformer를 결합한 아키텍처입니다. 주로 3D scene understanding, reconstruction, or generation에 사용된다.

이미지 상 투영을 통해 3d keypoint를 2d 위치로 투영(projection)시킨다.
이때 feature sampling을 한다.

View/Level fusion	deformable 모듈을 통해 멀티 카메라/멀티 레벨에서 정보 수집


Temporal 강화	특정 단계부터 time-aware refinement 수행 가능


SparseGaussian3DKeyPointsGenerator: anchor를 중심으로 3D Gaussian keypoints 생성

DeformableFeatureAggregation: 각 Gaussian keypoint 위치에서 multi-view feature 추출 및 집계

이 encoder는 위 keypoint와 feature 기반으로 anchor를 개선한다

sparse conv : 특정 위치에만 feature이 존재하므로 활성화된 위치만 계산한다 연상량이 매우 적고 메모리가 효율적으로 사용된다.

dense conv와 다르게 sparse conv는 의미있는 곳의 idx를 건네서 그 부분들만 계산한다.





 















